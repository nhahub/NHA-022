{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df4d0ce-52a0-4113-bd7b-ad5252dcc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cab83cd-4788-4c71-9f3d-840750d55f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PavementEye Stream\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"cassandra\")\\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6063b08-f117-4069-84ee-4f53710b7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka parameters\n",
    "kafka_bootstrap_servers = 'kafka:9092'  # kafka:9092 as we are inside the docker network\n",
    "kafka_topic = 'test' # Can be changed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1e4acd-fc50-45e2-a999-01cb9d573fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from Kafka\n",
    "kafka_stream_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2355d6a8-89de-421e-998f-48169c3c9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to see the right parsed value of the message\n",
    "parse_kafka_stream = kafka_stream_df.selectExpr('CAST(value as STRING) as json_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e725622-c076-4fca-8a2f-6fd72e6848c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for the incoming JSON messages\n",
    "schema = StructType([\n",
    "    StructField(\"lon\", DoubleType()),\n",
    "    StructField(\"lat\", DoubleType()),\n",
    "    StructField(\"time\", StringType()),\n",
    "    StructField(\"ppm\", DoubleType()),\n",
    "    StructField(\"image\", StringType()),\n",
    "    StructField(\"labels\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"label\", StringType()),\n",
    "            StructField(\"confidence\", DoubleType()),\n",
    "            StructField(\"x1\", DoubleType()),\n",
    "            StructField(\"x2\", DoubleType()),\n",
    "            StructField(\"y1\", DoubleType()),\n",
    "            StructField(\"y2\", DoubleType())\n",
    "        ])\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53a86f4-1791-4165-9fdc-a6fa76e7486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse JSON string into a structured DataFrame\n",
    "json_df = parse_kafka_stream.select(from_json(col(\"json_value\"), schema).alias(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0520870d-e9c8-431a-b728-3881850e1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explode the labels array so each detected object becomes one row\n",
    "exploded_df = json_df.select(\n",
    "    col(\"data.lon\"),\n",
    "    col(\"data.lat\"),\n",
    "    col(\"data.time\"),\n",
    "    col(\"data.ppm\"),\n",
    "    col(\"data.image\"),\n",
    "    explode(col(\"data.labels\")).alias(\"label_struct\")\n",
    ").select(\n",
    "    col(\"lon\"),\n",
    "    col(\"lat\"),\n",
    "    col(\"image\"),\n",
    "    col(\"time\").alias(\"timestamp\"),\n",
    "    col(\"ppm\"),\n",
    "    col(\"label_struct.label\").alias(\"label\"),\n",
    "    col(\"label_struct.confidence\").alias(\"confidence\"),\n",
    "    col(\"label_struct.x1\").alias(\"x1\"),\n",
    "    col(\"label_struct.x2\").alias(\"x2\"),\n",
    "    col(\"label_struct.y1\").alias(\"y1\"),\n",
    "    col(\"label_struct.y2\").alias(\"y2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17838f6a-121a-499a-bdb8-5e7db3995f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove empty values\n",
    "df_no_nulls = exploded_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1576fef-0171-4cc8-bba6-c3dba09f3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time' column from string to timestamp\n",
    "df_no_nulls = df_no_nulls.withColumn(\"timestamp\", F.col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Now deduplicate only on the last 1 minutes\n",
    "cleaned_df = df_no_nulls \n",
    "#     .withWatermark(\"timestamp\", \"1 minutes\") \\\n",
    "#     .dropDuplicates([\"label\", \"x1\", \"y1\", \"x2\", \"y2\", \"lon\", \"lat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da6b271-6926-45dd-baf2-f8b7292f32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify the correctness of the coordinates\n",
    "df_valid_coords = cleaned_df.filter(\n",
    "    (col(\"x1\") < col(\"x2\")) &\n",
    "    (col(\"y1\") < col(\"y2\")) &\n",
    "    (col(\"lon\") >= -180) & (col(\"lon\") <= 180) &  # التأكد من حدود longitude\n",
    "    (col(\"lat\") >= -90) & (col(\"lat\") <= 90)      # التأكد من حدود latitude\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "873cc61e-bab6-42cb-ba71-16081bf85ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inster the id (identifier for the crack)\n",
    "df_valid_coords = df_valid_coords.withColumn(\"id\", expr(\"uuid()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045af9c1-14d7-4d84-8497-5aa950e2cf65",
   "metadata": {},
   "source": [
    "# Join with OSM\n",
    "This enables us to get road, government, district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4508efd1-fe65-4b59-9b2c-36caea8a9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load roads dataset\n",
    "roads_df = gpd.read_file('../data/egypt/geo.geojson').to_crs(epsg=4326)\n",
    "roads_df = roads_df.drop(['index'], axis=1)\n",
    "roads_broadcast = spark.sparkContext.broadcast(roads_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72f0d677-a30b-41be-a1e3-53e19a570607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_roads(lon, lat):\n",
    "    # Import necessary libraries inside the UDF for execution on workers\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "    \n",
    "    # Access the broadcasted roads data\n",
    "    roads_df_local = roads_broadcast.value\n",
    "\n",
    "    location = Point(lon, lat)\n",
    "    stream_geo_df = gpd.GeoDataFrame(geometry=[location], crs=\"EPSG:4326\")\n",
    "\n",
    "    # Perform the nearest-neighbor spatial join\n",
    "    joined_data = gpd.sjoin_nearest(stream_geo_df, roads_df_local, how='inner', max_distance=20)\n",
    "\n",
    "    if not joined_data.empty:\n",
    "        # Return the index of the nearest road (scalar)\n",
    "        return int(joined_data.iloc[0]['index_right'])\n",
    "    else:\n",
    "        # Return a default value when no match is found\n",
    "        return -1\n",
    "\n",
    "join_roads = udf(join_roads, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "661b84da-459d-4d56-a545-916d48589e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(lon, lat):\n",
    "    # Import necessary libraries inside the UDF for execution on workers\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "    \n",
    "    # Access the broadcasted roads data\n",
    "    roads_df_local = roads_broadcast.value\n",
    "\n",
    "    location = Point(lon, lat)\n",
    "    stream_geo_df = gpd.GeoDataFrame(geometry=[location], crs=\"EPSG:4326\")\n",
    "\n",
    "    # Perform the nearest-neighbor spatial join\n",
    "    joined_data = gpd.sjoin_nearest(stream_geo_df, roads_df_local, how='inner', max_distance=20)\n",
    "\n",
    "    if not joined_data.empty:\n",
    "        # Return the index of the nearest road (scalar)\n",
    "        return joined_data.iloc[0]['ADM2_EN']\n",
    "    else:\n",
    "        # Return a default value when no match is found\n",
    "        return \"Unkown\"\n",
    "\n",
    "get_dist = udf(get_dist, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b153dc37-1efb-4862-9ec0-3f25eddb1950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_roads = df_valid_coords\\\n",
    "    .withColumn(\"road_index\", join_roads(col(\"lon\"), col(\"lat\")))\\\n",
    "    .withColumn(\"dist\", get_dist(col(\"lon\"), col(\"lat\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed5a4c87-b4f8-4ab4-b7d6-0f9cfd0c0f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# To insert the stream into cassandra database\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[43mdf_with_roads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.cassandra\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrack\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpavementeye\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/checkpoint4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# To insert the stream into cassandra database\n",
    "df_with_roads.writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"crack\", keyspace=\"pavementeye\")\\\n",
    "    .option('checkpointLocation', '/tmp/checkpoint4')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b37ba-3d87-45e1-a563-3ba247df8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for testing (printing in the notebook)\n",
    "df_with_roads.writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .foreachBatch(lambda batch_df, batch_id: batch_df.show(truncate=False))\\\n",
    "    .start()\\\n",
    "    .awaitTermination()\n",
    "\n",
    "# See value at the docker logs\n",
    "# When request is made you can see the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a4dee-76c5-4e53-a601-df611fe5e05b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
